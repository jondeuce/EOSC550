{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting, overfitting, regularization, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and minibatch MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, Plots, Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data, show some samples\n",
    "include(Knet.dir(\"data\",\"mnist.jl\"))\n",
    "xtrn,ytrn,xtst,ytst = mnist()\n",
    "for a in (xtrn,ytrn,xtst,ytst); println(summary(a)); end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some samples\n",
    "for i=1:3; display(mnistview(xtst,i)); end\n",
    "ytst[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch data\n",
    "Atype = gpu() >= 0 ? KnetArray{Float32} : Array{Float32}\n",
    "dtst = minibatch(xtst,ytst,100;xtype=Atype) # [ (x1,y1), (x2,y2), ... ] where xi,yi are minibatches of 100\n",
    "dtrn = minibatch(xtrn,ytrn,100;xtype=Atype) # [ (x1,y1), (x2,y2), ... ] where xi,yi are minibatches of 100\n",
    "length(dtrn),length(dtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the first minibatch\n",
    "(x,y)=first(dtst)\n",
    "println(summary(x))\n",
    "println(summary(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function linear(w,x)\n",
    "    y = w[1]*mat(x) .+ w[2]\n",
    "end\n",
    "\n",
    "winit1()=map(Atype, [ 0.1*randn(10,784), zeros(10,1) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srand(9)\n",
    "w = winit1()  # random weight matrix and a zero bias vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = linear(w,x)\n",
    "summary(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(ypred,y)  # accuracy on this batch of 100 with initial w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(w,dtst,linear)  # accuracy on the whole test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeroone loss (error) defined as 1 - accuracy\n",
    "zeroone(w,data,model) = 1 - accuracy(w,data,model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cross entropy loss of a model with weights w for one minibatch (x,y)\n",
    "# Use non-zero l1 or l2 for regularization (only on matrices not biases)\n",
    "function softloss(w,x,y,predict;l1=0,l2=0,o...)\n",
    "    J = nll(predict(w,x;o...),y)\n",
    "    if l1 != 0; J += l1 * sum(sum(abs,wi)  for wi in w[1:2:end]); end\n",
    "    if l2 != 0; J += l2 * sum(sum(abs2,wi) for wi in w[1:2:end]); end\n",
    "    return J\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softloss(w,x,y,linear)  # per-instance average softloss for the first test minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function avgloss(w,data,predict) # average loss for the whole dataset\n",
    "    sum = cnt = 0\n",
    "    for (x,y) in data\n",
    "        sum += softloss(w,x,y,predict)\n",
    "        cnt += 1\n",
    "    end\n",
    "    return sum/cnt\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgloss(w,dtst,linear)  # per-instance average softloss for the whole test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually defined gradient for softloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function grad1(w,x,y,predict)\n",
    "    x = mat(x)\n",
    "    p = predict(w,x)\n",
    "    p = p .- maximum(p,1) # for numerical stability\n",
    "    expp = exp.(p)\n",
    "    p = expp ./ sum(expp,1)\n",
    "    q = oftype(p, sparse(convert(Vector{Int},y),1:length(y),1,size(p,1),length(y)))\n",
    "    dJdy = (p - q) / size(x,2)\n",
    "    dJdw = dJdy * x'\n",
    "    dJdb = sum(dJdy,2)\n",
    "    Any[dJdw,dJdb]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically defined gradient for softloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softgrad = grad(softloss)  # Knet/AutoGrad makes life easier :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = grad1(w,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = softgrad(w,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isapprox(g1[1],g2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "isapprox(g1[2],g2[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(g1[2]')  \n",
    "# Meaning of gradient:\n",
    "# If I move the last entry of w[2] by epsilon, loss will go up by 0.345075 epsilon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(w[2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softloss(w,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w[2][10] += 0.1   # to numerically check the gradient let's move the last entry by +0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softloss(w,x,y,linear)  \n",
    "# We see that the loss moves by +0.03 as expected.\n",
    "# You should check all/most entries in your gradients this way to make sure they are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model(w) with SGD and return a list containing w for every epoch\n",
    "function train(w,data,model; epochs=100,lr=0.5,o...)\n",
    "    weights = Any[copy(w)]\n",
    "    for epoch in 1:epochs\n",
    "        for (x,y) in data\n",
    "            g = softgrad(w,x,y,model;o...)\n",
    "            for i in 1:length(w)\n",
    "                w[i] = w[i] - lr * g[i]\n",
    "            end\n",
    "        end\n",
    "        push!(weights,copy(w))\n",
    "    end\n",
    "    return weights\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srand(1)\n",
    "@time trn1=train(winit1(),dtrn,linear)\n",
    "@time trnloss1 = [ avgloss(w,dtrn,linear) for w in trn1 ]\n",
    "@time tstloss1 = [ avgloss(w,dtst,linear) for w in trn1 ]\n",
    "@time trnerr1 = [ zeroone(w,dtrn,linear) for w in trn1 ]\n",
    "@time tsterr1 = [ zeroone(w,dtst,linear) for w in trn1 ]\n",
    "minimum(tstloss1),minimum(tsterr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(trn1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnloss1 tstloss1],ylim=(.2,.36),labels=[:trnloss :tstloss],xlabel=\"Epochs\",ylabel=\"Loss\") \n",
    "# Demonstrates both overfitting and underfitting\n",
    "# Overfitting: test loss is higher than training loss and getting worse\n",
    "# Underfitting: training loss not close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerr1 tsterr1],ylim=(.06,.10),labels=[:trnerr :tsterr],xlabel=\"Epochs\",ylabel=\"Error\")  \n",
    "# this is the error plot, we get to about 8% error, i.e. 92% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us try L2 regularization to address overfitting\n",
    "srand(1)\n",
    "@time trn2=train(winit1(),dtrn,linear;l2=0.00004)\n",
    "@time trnloss2 = [avgloss(w,dtrn,linear) for w in trn2]\n",
    "@time tstloss2 = [avgloss(w,dtst,linear) for w in trn2]\n",
    "@time trnerr2 = [zeroone(w,dtrn,linear) for w in trn2]\n",
    "@time tsterr2 = [zeroone(w,dtst,linear) for w in trn2]\n",
    "minimum(tstloss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnloss1 tstloss1 trnloss2 tstloss2],ylim=(0.2,0.36),\n",
    "    labels=[:trnloss :tstloss :trnlossL2 :tstlossL2],xlabel=\"Epochs\",ylabel=\"Loss\") \n",
    "# overfitting less but results do not improve much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerr1 tsterr1 trnerr2 tsterr2],ylim=(0.06,0.10),\n",
    "    labels=[:trnerr :tsterr :trnerrL2 :tsterrL2],xlabel=\"Epochs\",ylabel=\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a model with higher capacity helps underfitting\n",
    "function mlp(w,x)\n",
    "    x = mat(x)\n",
    "    for i=1:2:length(w)-2\n",
    "        x = relu.(w[i]*x .+ w[i+1])\n",
    "    end\n",
    "    return w[end-1]*x .+ w[end]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function winit(h...; std=0.01, x=784, y=10,  # use winit(h1,h2,...,hn) for n hidden layer mlp\n",
    "               atype=gpu()>=0 ? KnetArray{Float32} : Array{Float32})\n",
    "    h = [x, h..., y]\n",
    "    w = Any[]\n",
    "    for i=1:length(h)-1\n",
    "        push!(w, std*randn(h[i+1],h[i]))\n",
    "        push!(w, zeros(h[i+1],1))\n",
    "    end\n",
    "    map(atype, w)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2=winit(64) # gives weights and biases for an MLP with a single hidden layer of size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softloss(w2,x,y,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srand(1)\n",
    "@time trn3=train(winit(64),dtrn,mlp)\n",
    "@time trnloss3 = [ avgloss(w,dtrn,mlp) for w in trn3 ]\n",
    "@time tstloss3 = [ avgloss(w,dtst,mlp) for w in trn3 ]\n",
    "@time trnerr3 = [ zeroone(w,dtrn,mlp) for w in trn3 ]\n",
    "@time tsterr3 = [ zeroone(w,dtst,mlp) for w in trn3 ]\n",
    "minimum(tstloss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnloss1 tstloss1 trnloss3 tstloss3],ylim=(0.0,0.4),\n",
    "    labels=[:trnLin :tstLin :trnMLP :tstMLP],xlabel=\"Epochs\",ylabel=\"Loss\")  \n",
    "# solves the underfitting problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerr1 tsterr1 trnerr3 tsterr3],ylim=(0,0.1),\n",
    "    labels=[:trnLin :tstLin :trnMLP :tstMLP],xlabel=\"Epochs\",ylabel=\"Error\")  \n",
    "# error improves from 8% to 2%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still have overfitting, let's try L1 regularization and a lower learning rate\n",
    "srand(1)\n",
    "@time trn4=train(winit(64),dtrn,mlp;lr=0.1,l1=4e-5)\n",
    "@time trnloss4 = [ avgloss(w,dtrn,mlp) for w in trn4 ]\n",
    "@time tstloss4 = [ avgloss(w,dtst,mlp) for w in trn4 ]\n",
    "@time trnerr4 = [ zeroone(w,dtrn,mlp) for w in trn4 ]\n",
    "@time tsterr4 = [ zeroone(w,dtst,mlp) for w in trn4 ]\n",
    "minimum(tstloss4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnloss3 tstloss3 trnloss4 tstloss4],ylim=(0,0.15),\n",
    "    labels=[:trnMLP :tstMLP :trnMLP_L1 :tstMLP_L1],xlabel=\"Epochs\", ylabel=\"Loss\")  \n",
    "# overfitting less, loss results improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerr3 tsterr3 trnerr4 tsterr4],ylim=(0,0.04),\n",
    "    labels=[:trnMLP :tstMLP :trnMLP_L1 :tstMLP_L1],xlabel=\"Epochs\", ylabel=\"Error\")    \n",
    "# however error results do not improve! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(tsterr3),minimum(tsterr4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout is another way to address overfitting\n",
    "function mlpdrop(w,x; pdrop=(0,0))\n",
    "    x = mat(x)\n",
    "    x = dropout(x,pdrop[1])\n",
    "    for i=1:2:length(w)-2\n",
    "        x = relu.(w[i]*x .+ w[i+1])\n",
    "        x = dropout(x,pdrop[2])\n",
    "    end\n",
    "    return w[end-1]*x .+ w[end]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srand(1)\n",
    "@time trn5=train(winit(64),dtrn,mlpdrop;lr=0.1,pdrop=(0.2,0))\n",
    "@time trnloss5 = [ avgloss(w,dtrn,mlpdrop) for w in trn5 ]\n",
    "@time tstloss5 = [ avgloss(w,dtst,mlpdrop) for w in trn5 ]\n",
    "@time trnerr5 = [ zeroone(w,dtrn,mlpdrop) for w in trn5 ]\n",
    "@time tsterr5 = [ zeroone(w,dtst,mlpdrop) for w in trn5 ]\n",
    "minimum(tstloss5),minimum(tsterr5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnloss3 tstloss3 trnloss5 tstloss5],ylim=(0,0.15),\n",
    "    labels=[:trnMLP :tstMLP :trnDropout :tstDropout],xlabel=\"Epochs\", ylabel=\"Loss\")  \n",
    "# overfitting less, loss results improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerr3 tsterr3 trnerr5 tsterr5],ylim=(0,0.04),\n",
    "    labels=[:trnMLP :tstMLP :trnDropout :tstDropout],xlabel=\"Epochs\", ylabel=\"Error\")  \n",
    "# this time error also improves slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(tsterr3),minimum(tsterr4),minimum(tsterr5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(tstloss3),minimum(tstloss4),minimum(tstloss5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with larger hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current trend is to use models with higher capacity tempered with dropout\n",
    "srand(1)\n",
    "@time trn6=train(winit(256),dtrn,mlpdrop;lr=0.1,pdrop=(0.2,0))\n",
    "@time trnloss6 = [ avgloss(w,dtrn,mlpdrop) for w in trn6 ]\n",
    "@time tstloss6 = [ avgloss(w,dtst,mlpdrop) for w in trn6 ]\n",
    "@time trnerr6 = [ zeroone(w,dtrn,mlpdrop) for w in trn6 ]\n",
    "@time tsterr6 = [ zeroone(w,dtst,mlpdrop) for w in trn6 ]\n",
    "minimum(tstloss6),minimum(tsterr6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnloss5 tstloss5 trnloss6 tstloss6],ylim=(0,0.15),\n",
    "    labels=[:trn64 :tst64 :trn256 :tst256],xlabel=\"Epochs\",ylabel=\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerr5 tsterr5 trnerr6 tsterr6],ylim=(0,0.04),\n",
    "    labels=[:trn64 :tst64 :trn256 :tst256],xlabel=\"Epochs\",ylabel=\"Error\")\n",
    "# We are down to 0.015 error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.1",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
